{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sc2-Yr039uyo"
   },
   "source": [
    "# Final Project: Monocular depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrSILBqo-aCF"
   },
   "source": [
    "## 1. Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojIPYMoW-cga"
   },
   "source": [
    "Self-supervised single-image (monocular) depth-estimation using neural-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FoDy_GY-fi8"
   },
   "source": [
    "## 2. Team Members and Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9p2-gpXL_LoX"
   },
   "source": [
    "- Alex Wang (j2373wan@uwaterloo.ca)\n",
    "\n",
    "- Suyeong Choi ()\n",
    "\n",
    "- Colby Wang ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfiebb2m-3sN"
   },
   "source": [
    "## 3. Code Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdjUHpTt_Ouk"
   },
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EuDk7A9_DWS"
   },
   "source": [
    "## 4. Demo for Single Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries, define global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.image as image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from Unet import UNet\n",
    "from loss_function import MonocularDepthLoss\n",
    "from dataloader import CustomDataset\n",
    "\n",
    "DATASET_PATH = 'dataset'\n",
    "LEFT = 'image_L'\n",
    "RIGHT = 'image_R'\n",
    "DISPARITY = 'disparity'\n",
    "\n",
    "image_shape = image.imread(\"dataset/image_L/2018-07-11-14-48-52_2018-07-11-15-09-57-367.png\").shape\n",
    "HEIGHT = 800\n",
    "WIDTH = 1760\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_transform_single(img, height, width):\n",
    "    \n",
    "    # Make sure all the image sample has same size defined by (height, width) by cutting the image or filling 0s\n",
    "    result = np.zeros((height, width, 3), dtype=np.float32)\n",
    "    img_height = img.shape[0]\n",
    "    img_width = img.shape[1]\n",
    "    min_height = min(img_height, height)\n",
    "    min_width = min(img_width, width)\n",
    "    result[:min_height, :min_width,:] = img[:min_height, :min_width,:]\n",
    "    \n",
    "    # The returned value should have 3 matrices for each channel\n",
    "    return np.array([result[:,:,0], result[:,:,1], result[:,:,2]])\n",
    "\n",
    "def disparity_transform(disparity, height, width):\n",
    "    result = np.zeros((height, width), dtype=np.float32)\n",
    "    d_height = disparity.shape[0]\n",
    "    d_width = disparity.shape[1]\n",
    "    min_height = min(d_height, height)\n",
    "    min_width = min(d_width, width)\n",
    "    result[:min_height, :min_width] = disparity[:min_height, :min_width]\n",
    "    return result\n",
    "\n",
    "def image_transform(sample):\n",
    "    result_sample = {LEFT: image_transform_single(sample[LEFT], HEIGHT, WIDTH)}\n",
    "    if sample[RIGHT] is not None:\n",
    "        result_sample[RIGHT] = image_transform_single(sample[RIGHT], HEIGHT, WIDTH)\n",
    "        result_sample[DISPARITY] = disparity_transform(sample[DISPARITY], HEIGHT, WIDTH)\n",
    "    return result_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "im_left = image.imread(\"images/stereo_pairs/tsukuba/scene1.row3.col3.ppm\")\n",
    "im_gt = image.imread(\"images/stereo_pairs/tsukuba/truedisp.row3.col3.pgm\")\n",
    "im_right = image.imread(\"images/stereo_pairs/tsukuba/scene1.row3.col4.ppm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the U-Net model and loss function\n",
    "# model = UNet()\n",
    "# loss_function = MonocularDepthLoss()\n",
    "\n",
    "# # Create a dummy left image and right image (single image, HWC format)\n",
    "# image_l = image_transform_single(im_left, im_left.shape[0], im_left.shape[1])\n",
    "# image_r = image_transform_single(im_right, im_right.shape[0], im_right.shape[1])\n",
    "# image_l = torch.from_numpy(np.expand_dims(image_l, axis=0)).to(device)\n",
    "# image_r = torch.from_numpy(np.expand_dims(image_r, axis=0)).to(device)\n",
    "\n",
    "# # Define the optimizer\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Number of epochs to train (set high to see if it can overfit)\n",
    "# epochs = 100\n",
    "\n",
    "# # List to record losses\n",
    "# loss_history = []\n",
    "\n",
    "# # Training loop\n",
    "# model.to(device)\n",
    "# model.train()  # Set model to training mode\n",
    "# for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # Forward pass\n",
    "#     right_disparity, left_disparity = model(image_l)\n",
    "\n",
    "#     # Calculate loss\n",
    "#     loss = loss_function(left_disparity, image_r, image_l, right_disparity)\n",
    "#     loss_history.append(loss.item())\n",
    "\n",
    "#     # Backward pass and optimize\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# print(\"Training complete. The model should have overfit to the single image.\")\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(loss_history, label='Training Loss')\n",
    "# plt.title('Training Loss Over Epochs')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# # Save the plot as a PNG file\n",
    "# #plt.savefig('training_loss.png')  # You can specify a different path or file name\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# out_r_dmap, out_l_dmap = model(image_l)\n",
    "# output = out_l_dmap.detach().cpu()\n",
    "# output = torch.Tensor.numpy(output)\n",
    "# output = output.reshape(output.shape[2], output.shape[3])\n",
    "\n",
    "# fig = plt.figure(figsize = (12, 10))\n",
    "# plt.subplot(221)\n",
    "# plt.title(\"left image\")\n",
    "# plt.imshow(im_left)\n",
    "# plt.subplot(222) \n",
    "# plt.title(\"right image\")\n",
    "# plt.imshow(im_right)\n",
    "# plt.subplot(223)\n",
    "# plt.title(\"ground truth disparity map\")\n",
    "# plt.imshow(im_gt)\n",
    "# plt.colorbar(cax=plt.axes([0.91, 0.557, 0.015, 0.3]))\n",
    "# plt.subplot(224)\n",
    "# plt.title(\"model output on the training data\")\n",
    "# plt.imshow(output)\n",
    "# plt.colorbar(cax=plt.axes([0.91, 0.557, 0.015, 0.3]))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo for batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 8.04 GiB is allocated by PyTorch, and 411.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m right_disparity, left_disparity \u001b[38;5;241m=\u001b[39m model(image_l)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(left_disparity, image_r, image_l, right_disparity)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\uw_resources\\4B\\CS 484\\Final\\The-fun-project\\Unet.py:125\u001b[0m, in \u001b[0;36mUNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    122\u001b[0m up4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_upsample4(dec3)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# The initial image 'x' is concatenated in the last layer\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m dec4 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((up4, x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    126\u001b[0m dec4a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_conv4a(dec4)\n\u001b[0;32m    127\u001b[0m dec4b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_conv4b(dec4)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 8.04 GiB is allocated by PyTorch, and 411.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Initialize DataLoader\n",
    "training_dataset = CustomDataset(root_dir=DATASET_PATH, transform=image_transform)\n",
    "train_loader = DataLoader(training_dataset, batch_size=5, shuffle=True)  # Set a valid batch size\n",
    "\n",
    "# Initialize the U-Net model and loss function\n",
    "model = UNet()\n",
    "loss_function = MonocularDepthLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs to train (set high to see if it can overfit)\n",
    "epochs = 100\n",
    "\n",
    "# List to record losses\n",
    "loss_history = []\n",
    "\n",
    "# Training Loop\n",
    "model.to(device)\n",
    "model.train() # Set model to training mode\n",
    "\n",
    "\n",
    "# Testing for 1 epoch\n",
    "for i, sample in enumerate(train_loader):\n",
    "    print(i)\n",
    "    image_l = sample[LEFT].to(device)\n",
    "    image_r = sample[RIGHT].to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    right_disparity, left_disparity = model(image_l)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = loss_function(left_disparity, image_r, image_l, right_disparity)\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "#     epoch_loss = 0.0\n",
    "#     for i, sample in enumerate(train_loader):\n",
    "#         print(i)\n",
    "#         image_l, image_r = sample[LEFT], sample[RIGHT]\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         right_disparity, left_disparity = model(image_l)\n",
    "\n",
    "#         # Calculate loss\n",
    "#         loss = loss_function(left_disparity, image_r, image_l, right_disparity)\n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#         # Backward pass and optimize\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#     epoch_loss /= len(dl)\n",
    "#     loss_history.append(epoch_loss)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_history, label='Training Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWMiRr2v_ADG"
   },
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmxxB6eY_S39"
   },
   "source": [
    "conclusions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
